\section{Conclusion}
This section gives a brief overall summary of the entire project as well as the assumptions made in the methods are also discussed alongwith ideas for improvement.

\subsection{Summary}
In this paper, we first introduce the FTCS method and briefly describe its usage in solving parabolic PDEs. We first discuss the serial implementation of the method, within which we discuss stability conditions to choose appropratie $\tau$ and $h$ values. Next, the parallellization of the method is discussed, where we demonstrate how domain decomposition is done, and how communication within the processes is established. The results for both methodologies are shown and the improvement in execution times are discussed.

\subsection{Uneven Distrbution: \texttt{Scatterv()}}
The minimum step-size $h$ used in the serial method is not used in the parallel method, reason being that we wanted even distribution of the decomposed domain and also to always keep the number of processes as a power of 2. This is a voluntary choice to easily distribute the decomposition through a simple method \texttt{Scatter()}. However, if further customization is needed, we can make the distribution uneven as well using \texttt{Scatterv()}, for this the first \texttt{rank} will the remainder of the decomposed domain. The uneven distribution allows us to use any number of processes, with no issues in using the same minimum step-size as in the serial method.

\subsection{Final Thoughts}
The overall project was exciting as well as very informative. We, however, find using MPI in C/C++ a bit more clear and explicit as opposed to \texttt{mpi4py} in Python which we found a bit less intuitive and abstract. But this hands-on approach to an MPI Project really made us appreciate the usefulness of parallelization in research projects as well as numerical methods.